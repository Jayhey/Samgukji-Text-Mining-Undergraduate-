a <- mat %*% t(mat)
write.csv(a, "gg.csv")
library(igraph)
g <- graph_from_adjacency_matrix(a, mode = "undirected",
weighted = TRUE, diag = FALSE)
plot.igraph(g)
V(g)
length(V(g))
g1
faction1 <- c("하후돈","조조","허저","정욱","방덕","서황","장료")
faction2 <- c("위연","방통",
"마초","유비","유표","장비","조운","관우",
"제갈량","황충")
faction3 <- c("주유","노숙","손권","육손","여몽", "제갈근")
length(faction1)+length(faction2)+length(faction3)
faction1 <- c("하후돈","조조","허저","정욱","방덕","서황","장료")
faction2 <- c("위연","방통",
"마초","유비","유표","장비","조운","관우",
"제갈량","황충")
faction3 <- c("주유","노숙","손권","육손","여몽", "제갈근")
V(g)$shape <- "circle"
V(g)[c("조조", "유비","손권")]$shape <- "rectangle"
V(g)[faction1]$color <- "red"
V(g)[faction2]$color <- "dodgerblue"
V(g)[faction3]$color <- "green"
faction3 <- c("주유","노숙","손권","육손","여몽")
V(g)[faction3]$color <- "green"
x <- E(g)$weight
E(g)$width <- x
plot.igraph(g)
graph.strength(g)
graph.strength(g)
V(g)$size <- 5*sqrt(graph.strength(g))
plot.igraph(g)
V(g)$size <- 4*sqrt(graph.strength(g))
plot.igraph(g)
plot.igraph(g)
F1 <- V(g)[faction1]
F2 <- V(g)[faction2]
F3 <- V(g)[faction3]
E(g)[ F1 %--% F1 ]$color <- "pink"
E(g)[ F2 %--% F2 ]$color <- "lightblue"
E(g)[ F3 %--% F3 ]$color <- "lightgreen"
plot.igraph(g)
plot.igraph(g)
E(g)[ F1 %--% F2 ]$color <- "orangered1"
E(g)[ F1 %--% F3 ]$color <- "yellow"
E(g)[ F2 %--% F3 ]$color <- "cyan"
plot.igraph(g)
E(g)[ F1 %--% F3 ]$color <- "yellow2"
plot.igraph(g)
V(g)$size <- 3*sqrt(graph.strength(g))
plot.igraph(g)
plot.igraph(g)
V(g)$label.dist <- ifelse(V(karate)$size >= 10, 0, 0.75)
plot.igraph(g)
V(g)$label.dist <- ifelse(V(g)$size >= 10, 0, 0.75)
plot.igraph(g)
V(g)$size
V(g)$label.dist <- ifelse(V(g)$size >= 9, 0, 0.75)
plot.igraph(g)
g <- graph_from_adjacency_matrix(a, mode = "undirected",
weighted = TRUE, diag = FALSE)
faction1 <- c("하후돈","조조","허저","정욱","방덕","서황","장료")
faction2 <- c("위연","방통",
"마초","유비","유표","장비","조운","관우",
"제갈량","황충")
faction3 <- c("주유","노숙","손권","육손","여몽")
plot.igraph(g)
V(g)$shape <- "circle"
V(g)[c("조조", "유비","손권")]$shape <- "rectangle"
V(g)[faction1]$color <- "red"
V(g)[faction2]$color <- "dodgerblue"
V(g)[faction3]$color <- "green"
plot.igraph(g)
V(g)$size <- 2*sqrt(graph.strength(g))
plot.igraph(g)
V(g)$label.dist <- ifelse(V(g)$size >= 9, 0, 0.75)
plot.igraph(g)
View(BG)
s6[485,]
V(g)$size <- 3*sqrt(graph.strength(g))
plot.igraph(g)
x <- E(g)$weight
E(g)$width <- x
plot.igraph(g)
x
E(g)$width <- sqrt(x)
plot.igraph(g)
setwd("C:/Users/winga.JAY/Desktop/R/캡스톤2")
library(tm)
library(stringi)
library(topicmodels)
install.packages("tm", dependencies = TRUE)
install.packages("topicmodels")
library(tm)
library(stringi)
library(topicmodels)
library(igraph)
load("INS.RData")
View(INS)
INS.Corpus <- Corpus(VectorSource(INS$Abstract))
INS.Corpus
INS.Corpus <- tm_map(INS.Corpus, content_transformer(stri_trans_tolower))
INS.Corpus <- tm_map(INS.Corpus, content_transformer(removePunctuation))
INS.Corpus <- tm_map(INS.Corpus, content_transformer(removeNumbers))
myStopwords <- c(stopwords("en"))
INS.Corpus <- tm_map(INS.Corpus, removeWords, myStopwords)
INS.Corpus <- tm_map(INS.Corpus, stemDocument)
myStopwords <- c(stopwords("en"), "propos", "method", "algorithm", "elsevi", "result", "right","reserv", "paper", "present","studi")
INS.Corpus <- tm_map(INS.Corpus, removeWords, myStopwords)
AbsDTM <- DocumentTermMatrix(INS.Corpus, control = list(minWordLength = 2))
NTopic <- 30 #토픽 갯수는 우리가 원하는 만큼 정해주자. 이거는 다 다름.
ptm <- proc.time() #alpha가 1이면 완벽하게 랜덤. 커지면 토픽 분포가 같게 하는 것. 작아지면 한 토픽 안에 소수의 토픽만 섞이게 하는 것. 1 아니면 0.1이 좋다고 하심.
control_LDA_Gibbs <- list(alpha = 0.1, estimate.beta = TRUE, verbose = 0, #beta는 1이면 모든 단어가 랜덤하게 분포. 1보다 작으면 핵심어가 도드라지게 만들겠다. estimate.beta는 최적값 만들어주는 것.
prefix = tempfile(), save = 0, keep = 0, #나머지는 help파일 찾아봐
seed = as.integer(Sys.time()), nstart = 1,
best = TRUE, delta = 0.1, iter = 2000, #iteration 1000~5000사이면 수렴이 됨.
burnin = 0, thin = 2000)
load("INS_LDA.RData")
load("INS_LDA.RData")
TermFileName <- paste("AssignedTerms_", NTopic, ".csv", sep="")
CoTableFileName <- paste("CoTable_", NTopic, ".csv", sep="")
Top5.PaperFileName <- paste("Top5Papers_", NTopic, ".csv", sep="")
Gibbs.terms <- terms(Gibbs_LDA, 30) #30개 어휘를 뽑아달라.
write.csv(Gibbs.terms, TermFileName)
write.csv(Gibbs.terms, TermFileName)
View(Gibbs.terms)
write.csv(BG,"bg.csv")
library(party)
library(ROCR)
setwd("C:/rr/sam")
setwd("C:/Users/winga.JAY/Desktop/R/캡스톤2")
ploan <- read.csv("Personal Loan.csv")
ploan <- read.csv("Personal Loan.csv")
View(ploan)
ploan.x <- ploan[,-c(1,5,10)]
ploan.y <- as.data.frame(as.factor(ploan[,10])) #파티 패키지는 팩ㅌ로 받아줘야 함.
trn_idx <- sample(1:dim(ploan.y)[1], round(0.7*dim(ploan.y)[1]))
ploan.trn <- cbind(ploan.x[trn_idx,], ploanYN = ploan.y[trn_idx,])
ploan.val <- cbind(ploan.x[-trn_idx,], ploanYN = ploan.y[-trn_idx,])
ploan.all <- rbind(ploan.trn, ploan.val)
min_criterion = c(0.9, 0.95, 0.99)
min_split = c(10, 30, 50, 100) #숫자가 작을수록 쉽게 분기가 된다. 100개가 안되면 100일 떄 시도조차 하지 않음.
max_depth = c(0, 10, 5) #숫자가 클수록 복잡한 트리. 0은 제한을 안주는거임.
tree_result = matrix(0,length(min_criterion)*length(min_split)*length(max_depth),9)
27
tree_result
iter_cnt = 1
for (i in 1:length(min_criterion))
{
for ( j in 1:length(min_split))
{
for ( k in 1:length(max_depth))
{
cat("CART Min criterion:", min_criterion[i], ", Min split:",
min_split[j], ", Max depth:", max_depth[k], "\n")
tmp_control = ctree_control(mincriterion = min_criterion[i],
minsplit = min_split[j], maxdepth = max_depth[k])
tmp_tree <- ctree(ploanYN ~ ., data = ploan.trn, controls = tmp_control)
tmp_tree_val_prediction <- predict(tmp_tree, newdata = ploan.val)
tmp_tree_val_response <- treeresponse(tmp_tree, newdata = ploan.val)
tmp_tree_val_prob <- 1-unlist(tmp_tree_val_response, use.names=F)[seq(1,nrow(ploan.val)*2,2)]
tmp_tree_val_rocr <- prediction(tmp_tree_val_prob, ploan.val$ploanYN)
tmp_tree_val_cm <- table(ploan.val$ploanYN, tmp_tree_val_prediction)
# parameters
tree_result[iter_cnt,1] = min_criterion[i]
tree_result[iter_cnt,2] = min_split[j]
tree_result[iter_cnt,3] = max_depth[k]
# Recall
Recall = tmp_tree_val_cm[2,2]/(tmp_tree_val_cm[2,1]+tmp_tree_val_cm[2,2])
tree_result[iter_cnt,4] = Recall
# Precision
Precision <- tmp_tree_val_cm[2,2]/(tmp_tree_val_cm[1,2]+tmp_tree_val_cm[2,2])
tree_result[iter_cnt,5] = Precision
# Accuracy
tree_result[iter_cnt,6] = (tmp_tree_val_cm[1,1]+tmp_tree_val_cm[2,2])/sum(tmp_tree_val_cm)
# F1 measure
tree_result[iter_cnt,7] = 2*Recall*Precision/(Recall+Precision)
# AUROC
tree_result[iter_cnt,8] = unlist(performance(tmp_tree_val_rocr, "auc")@y.values)
# Number of leaf nodes
tree_result[iter_cnt,9] = length(nodes(tmp_tree, unique(where(tmp_tree))))
iter_cnt = iter_cnt + 1
}
}
}
tree_result
tree_result <- tree_result[order(tree_result[,8], decreasing = T),]
best_criterion <- tree_result[1,1]
best_split <- tree_result[1,2]
best_depth <- tree_result[1,3]
tree_control = ctree_control(mincriterion = best_criterion, minsplit = best_split, maxdepth = best_depth)
tree <- ctree(ploanYN ~ ., data = ploan.all, controls = tree_control)
tree_all_prediction <- predict(tree, newdata = ploan.all)
tree_all_response <- treeresponse(tree, newdata = ploan.all)
tree_all_prob <- 1-unlist(tree_all_response, use.names=F)[seq(1,nrow(ploan.all)*2,2)]
tree_all_rocr <- prediction(tree_all_prob, ploan.all$ploanYN)
# Performance of the best tree
tree_all_cm <- table(ploan.all$ploanYN, tree_all_prediction)
best_result <- matrix(0,1,6)
# Recall
Recall = tree_all_cm[2,2]/(tree_all_cm[2,1]+tree_all_cm[2,2])
best_result[1,1] = Recall
# Precision
Precision <- tree_all_cm[2,2]/(tree_all_cm[1,2]+tree_all_cm[2,2])
best_result[1,2] = Precision
# Accuracy
best_result[1,3] = (tree_all_cm[1,1]+tree_all_cm[2,2])/sum(tree_all_cm)
# F1 measure
best_result[1,4] = 2*Recall*Precision/(Recall+Precision)
# AUROC
best_result[1,5] = unlist(performance(tree_all_rocr, "auc")@y.values)
# Number of leaf nodes
best_result[1,6] = length(nodes(tree, unique(where(tree))))
best_result
# Plot the ROC
tmp <- 1-unlist(tree_all_response, use.names=F)[seq(1,nrow(ploan.all)*2,2)]
tmp.rocr <- prediction(tmp, ploan.all$ploanYN)
tmp.perf <- performance(tmp.rocr, "tpr","fpr")
plot(tmp.perf, col=5, lwd = 3)
plot(tree)
plot(tree, type="simple")
print(tree)
plot(tree)
plot(tree, type="simple")
load("INS.RData")
View(INS)
setwd("C:/rr/sam")
library(tm)
library(stringi)
library(topicmodels)
library(igraph)
LDAtest <- read.csv("ldatest.csv")
LDAtest.Corpus <- Corpus(VectorSource(LDAtest$V2))
LDAtest.Corpus
INS.Corpus <- Corpus(VectorSource(INS$Abstract))
INS.Corpus
library(Konlp)
library(KONLP)
library(KoNLP)
install.packages("KoNLP")
install.packages("KoNLP")
.libPaths()
library(KoNLP)
install.packages("rjava")
install.packages("rJava")
library(KoNLP)
library(KoNLP)
C15 <- c(7,7,15,11,9)
C20 <- c(12,17,12,18,18)
C25 <- c(14,18,18,19,19)
C30 <- c(19,25,22,19,23)
C35 <- c(7,10,11,15,11)
y <- c(C15,C20,C25,C30,C35)
n = rep(5,5)
n
group = rep(1:5,n)
group
data=data.frame(y=y,group=factor(group))
fit=lm(y~group, data)
anova(fit)
plot(fit)
trt = c(rep(c("C1"),4),rep(c("C2"),4),rep(c("C3"),4))
data=cbind(figure,block,trt)
data=as.data.frame(data)
names(data)=c("figure","block","trt")
lm.out=lm(figure~block+trt)
anova(lm.out)
plot(lm.out)
figure=c(0.05,0.05,0.04,0.15,0.05,0.05,0.04,0.17,0.04,0.04,0.03,0.1)
block=rep(c("B1","B2","B3","B4"),3)
trt = c(rep(c("C1"),4),rep(c("C2"),4),rep(c("C3"),4))
data=cbind(figure,block,trt)
data=as.data.frame(data)
names(data)=c("figure","block","trt")
lm.out=lm(figure~block+trt)
anova(lm.out)
plot(lm.out)
n <- rep(5,5)
n
group = rep(1:5,n)
group
?lm
C15 <- c(7,7,15,11,9)
C20 <- c(12,17,12,18,18)
C25 <- c(14,18,18,19,19)
C30 <- c(19,25,22,19,23)
C35 <- c(7,10,11,15,11)
y <- c(C15,C20,C25,C30,C35)
y
data=data.frame(y=y,group=factor(group))
fit=lm(y~group, data)
data
anova(fit)
plot(fit)
group = rep(1:5,1:5)
group
group = rep(1:5,1:5)
group
data=data.frame(y=y,group=factor(group))
fit=lm(y~group, data)
group = rep(1:5,5)
group
?append
for(i in 1:5)
{
group <- append(group, i,5)
}
group
?rep
rep(1, 1:5)
rep(1, 15)
rep(1, 5)
C15 <- c(7,7,15,11,9)
C20 <- c(12,17,12,18,18)
C25 <- c(14,18,18,19,19)
C30 <- c(19,25,22,19,23)
C35 <- c(7,10,11,15,11)
C <- c(C15,C20,C25,C30,C35)
data5_77 <- data.frame(y = C, group=factor(group))
group5_77 <- rep(1:5,rep(5:5))
model5_77 <- lm(C ~ group5_77, data5_77)
#5_77
C15 <- c(7,7,15,11,9)
C20 <- c(12,17,12,18,18)
C25 <- c(14,18,18,19,19)
C30 <- c(19,25,22,19,23)
C35 <- c(7,10,11,15,11)
C <- c(C15,C20,C25,C30,C35)
group5_77 <- rep(1:5,rep(5:5))
data5_77 <- data.frame(y = C, group=factor(group))
model5_77 <- lm(C ~ group5_77, data5_77)
anova(model5_77)
#5_77
C15 <- c(7,7,15,11,9)
C20 <- c(12,17,12,18,18)
C25 <- c(14,18,18,19,19)
C30 <- c(19,25,22,19,23)
C35 <- c(7,10,11,15,11)
C <- c(C15,C20,C25,C30,C35)
group5_77 <- rep(1:5,rep(5:5))
data5_77 <- data.frame(y = C, group5_77=factor(group5_77))
model5_77 <- lm(C ~ group5_77, data5_77)
anova(model5_77)
plot(model5_77)
#5_77
C15 <- c(7,7,15,11,9)
C20 <- c(12,17,12,18,18)
C25 <- c(14,18,18,19,19)
C30 <- c(19,25,22,19,23)
C35 <- c(7,10,11,15,11)
C <- c(C15,C20,C25,C30,C35)
group5_77 <- rep(1:5,rep(5,5))
data5_77 <- data.frame(y = C, group5_77=factor(group5_77))
model5_77 <- lm(C ~ group5_77, data5_77)
anova(model5_77)
plot(model5_77)
figure=c(0.05,0.05,0.04,0.15,0.05,0.05,0.04,0.17,0.04,0.04,0.03,0.1)
block=rep(c("B1","B2","B3","B4"),3)
trt = c(rep(c("C1"),4),rep(c("C2"),4),rep(c("C3"),4))
data=cbind(figure,block,trt)
data=as.data.frame(data)
names(data)=c("figure","block","trt")
lm.out=lm(figure~block+trt)
anova(lm.out)
plot(lm.out)
trt
lm.out=lm(figure~block)
anova(lm.out)
mean(C15)
mean(C20)
mean(C25)
maen(C30)
mean(C30)
mean(C35)
library(stringi)
library(stringr)
library(rJava)
library(xlsx)
remove.packages("KoNLP")
install.packages("KoNLP")
library(KoNLP)
remove.packages("rJava")
install.packages("rJava")
library(KoNLP)
library(rJava)
load("C:/Users/winga.JAY/Desktop/test/samguk/실험돌린거.RData")
library(stringi)
library(stringr)
library(reshape2)
library(igraph)
library(KoNLP)
library(tm)
library(topicmodels)
mode(all_conv[1,2])
mode(all_conv[1,2])
mode(all_conv[1,2])
mode(aa2[1,2])
View(aa2)
myCorpus <- Corpus(VectorSource(aa2$V2))
choong <- readLines("choong.txt")
choong2 <- unique(choong)
myStopwords <- choong2
myCorpus <- tm_map(myCorpus, removePunctuation)
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, removeWords, choong2)
myCorpus$`3`
myCorpus$`4
''
''
''
f
ff
dvz
c
zc
v2
3
33
''
'''
''
3''
11
()
)
P\
]ksfdljs;jkl
myCorpus <- Corpus(VectorSource(aa2$V2))
choong <- readLines("choong.txt")
choong2 <- unique(choong)
myStopwords <- choong2
myCorpus <- tm_map(myCorpus, removePunctuation)
initRhino <- function(path)
{
setwd(paste(path, "/RHINO", sep=""))
if(!require(rJava)) {install.packages("rJava"); library(rJava)}
.jinit()
.jaddClassPath(paste(path, "/RHINO", sep=""))
.jclassPath()
RHINO <- .jnew("rhino/RHINO")
.jcall(RHINO, returnSig = "V", "ExternInit")
return(RHINO)
}
initRhino <- function(path)
{
setwd(paste(path, "/RHINO", sep=""))
if(!require(rJava)) {install.packages("rJava"); library(rJava)}
.jinit()
.jaddClassPath(paste(path, "/RHINO", sep=""))
.jclassPath()
RHINO <- .jnew("rhino/RHINO")
.jcall(RHINO, returnSig = "V", "ExternInit")
return(RHINO)
}
getMorph <- function(sentence, type)
{
result <- .jcall(RHINO, returnSig = "S", "getMorph", sentence, type)
Encoding(result) <- "UTF-8"
resultVec <- unlist(strsplit(result, '\r\n'))
return(resultVec)
}
getMorph("아버지는 나를 좋아하십니다", "noun")
path <- getwd()          #You must change the path by yours. Use getwd().
RHINO <- initRhino(path)
path <- getwd()          #You must change the path by yours. Use getwd().
getwd()
setwd("C:/Users/winga.JAY/Desktop/test/samguk")
path <- getwd()          #You must change the path by yours. Use getwd().
RHINO <- initRhino(path)
raw_conv <- read.csv("raw_conv_final.csv")
raw_conv <- read.csv("raw_conv_final.csv")
verb_and_adj <- function(pre_mid_conv){
vdj <- data.frame(name = pre_mid_conv[,1])
b <- list()
a <- lapply(pre_mid_conv[,2], extractNoun)
for(i in 1:length(a)){
for(j in 1:length(a[[i]])){
vdj[i,2] <- paste(a[[i]][j], vdj[i,2])
}
cat(i,"/",length(a),"  1st","\n")
}
b <- list()
c <- list()
for(i in 1:nrow(pre_mid_conv)){
b[[i]] <- getMorph(pre_mid_conv[i,2],"VV")
c[[i]] <- getMorph(pre_mid_conv[i,2],"VA")
cat(i,"/",nrow(pre_mid_conv),"  2nd","\n")
}
for(j in 1:nrow(pre_mid_conv)){
for(k in 1:length(b[[i]])){
b[[j]][k] <- paste(b[[j]][k], "다", sep="")
vdj[j,3] <- paste(b[[j]][k], vdj[j,3])
}
}
for(j in 1:nrow(pre_mid_conv)){
for(k in 1:length(c[[i]])){
vdj[j,3] <- paste(c[[j]][k], vdj[j,3])
}
}
return(vdj)
}
raw_conv_ex <- verb_and_adj(raw_conv)
library(stringi)
library(stringr)
library(KoNLP)
library(tm)
library(topicmodels)
library(KoNLP)
library(tm)
library(KoNLP)
